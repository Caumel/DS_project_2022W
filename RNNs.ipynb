{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN podemos usar, pero is complex to train with longer sequences and can suffer from the problem of vanishing gradients\n",
    "\n",
    "Hay otra variaciones como LSTM and GRU que no tienen este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 02:55:38.620291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-15 02:55:39.044291: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-15 02:55:39.044309: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-15 02:55:40.137407: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-15 02:55:40.137742: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-15 02:55:40.137753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files per wildboard and compute the areas points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE fiwi for train. becuase other in the positions we dont have too much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files_fiwi = \"../Join/telemetriedaten/positions_weather_fiwi/\"\n",
    "# path_files_other = \"../Join/telemetriedaten/positions_weather_other/\"\n",
    "\n",
    "path_place_fiwi = \"../Join/telemetriedaten/TelemetrieFiwigatter/\"\n",
    "# path_place_other = \"../Join/telemetriedaten/TelemetrieJagdgatter/\"\n",
    "\n",
    "save_file_path = \"../Join/telemetriedaten/position_per_wild_fiwi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = os.listdir(path_files_fiwi)\n",
    "wild_boards_fiwi = set()\n",
    "for file in list_files:\n",
    "    df = pd.read_csv(path_files_fiwi + file)\n",
    "    wild_boards_fiwi.update(list(df[\"ID\"].unique()))\n",
    "\n",
    "# list_files = os.listdir(path_files_other)\n",
    "# wild_boards_other = set()\n",
    "# for file in list_files:\n",
    "#     df = pd.read_csv(path_files_other + file)\n",
    "#     wild_boards_other.update(list(df[\"ID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_boards_fiwi = {\n",
    "    'VG.2013.01', 'IS.2011.13', 'FG.2013.16', 'FG.2016.16', 'FG.2013.32', 'FG.2017.12', 'LA.2011.01', 'IS.2011.27',\n",
    "    'IS.2011.05', 'VG.2013.10', 'IS.2011.20', 'FG.2013.29', 'FG.2016.14', 'FG.2017.11', 'IS.2011.29', 'FG.2016.13', 'IS.2011.10', \n",
    "    'IS.2011.37', 'FG.2016.15', 'FG.2016.17', 'FG.2013.09', 'LA.2011.03', 'DE.2011.19', 'IS.2011.21', 'IS.2011.39', 'FG.2017.09', \n",
    "    'DE.2011.27', 'IS.2011.32', 'DE.2011.23', 'VG.2013.09', 'FG.2017.18', 'IS.2011.12', 'IS.2011.26', 'IS.2011.25', 'IS.2011.02', \n",
    "    'IS.2011.23', 'FG.2016.12', 'FG.2016.03', 'DE.2011.21', 'IS.2011.04', 'IS.2011.11', 'DE.2011.18', 'FG.2016.05', 'FG.2016.07', \n",
    "    'FG.2017.16', 'FG.2016.18', 'FG.2017.13', 'IS.2011.22', 'FG.2013.01', 'FG.2013.06', 'VG.2013.02', 'IS.2011.36', 'FG.2017.14', \n",
    "    'DE.2011.20', 'FG.2017.17', 'IS.2011.15', 'IS.2011.34', 'DE.2011.14', 'VG.2013.08', 'IS.2011.14', 'DE.2011.25', 'IS.2011.01', \n",
    "    'IS.2011.38', 'FG.2013.34', 'FG.2013.25', 'IS.2011.33', 'VG.2013.11'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wild in wild_boards_fiwi:\n",
    "    if not os.path.exists(os.path.join(save_file_path,wild)):\n",
    "        os.mkdir(os.path.join(save_file_path,wild))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n",
      "/tmp/ipykernel_229/2223551429.py:3: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_files_fiwi + file)\n"
     ]
    }
   ],
   "source": [
    "list_files = os.listdir(path_files_fiwi)\n",
    "for file in list_files:\n",
    "    df = pd.read_csv(path_files_fiwi + file)\n",
    "    for wild in wild_boards_fiwi:\n",
    "        df_to_save = df[df[\"ID\"]==wild]\n",
    "        df_to_save.to_csv(os.path.join(save_file_path,wild,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file of areas\n",
    "\n",
    "file_areas = \"../Join/telemetriedaten/TelemetrieFiwigatter/TelemetrieFiwigatter_Tel.Fiwi.csv\"\n",
    "\n",
    "df = pd.read_csv(file_areas)\n",
    "df[\"start\"] = pd.to_datetime(df[\"start\"], infer_datetime_format=True)   \n",
    "df[\"end\"] = pd.to_datetime(df[\"end\"], infer_datetime_format=True)   \n",
    "\n",
    "# , \"ID\" \"location\",\"start\",\"end\",\"duration\"\n",
    "# 0,\"DE.2011.23\",\"Fiwi\",2017-02-01 15:55:08,2017-02-01 15:58:13,3.08333333333333\n",
    "\n",
    "df_2016 = df[(df[\"start\"] > datetime.strptime(\"2016-01-01\", '%Y-%m-%d')) & (df[\"start\"] < datetime.strptime(\"2016-12-31\", '%Y-%m-%d'))]\n",
    "df_2017 = df[(df[\"start\"] > datetime.strptime(\"2017-01-01\", '%Y-%m-%d')) & (df[\"start\"] < datetime.strptime(\"2017-12-31\", '%Y-%m-%d'))]\n",
    "df_2018 = df[(df[\"start\"] > datetime.strptime(\"2018-01-01\", '%Y-%m-%d')) & (df[\"start\"] < datetime.strptime(\"2018-12-31\", '%Y-%m-%d'))]\n",
    "df_2019 = df[(df[\"start\"] > datetime.strptime(\"2019-01-01\", '%Y-%m-%d')) & (df[\"start\"] < datetime.strptime(\"2019-12-31\", '%Y-%m-%d'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnamed: 0               0.0.5196\n",
    "# ID                     DE.2011.14\n",
    "# location                     Fiwi\n",
    "# start         2017-12-20 13:42:06\n",
    "# end           2017-12-20 13:42:06\n",
    "# duration                      0.0\n",
    "# Name: 78109, dtype: object\n",
    "\n",
    "df_final = pd.DataFrame(columns=[\"ID\",\"Date\",\"x-value\",\"y-value\",\"location\"])\n",
    "\n",
    "# for index_data,df in [df_2016,df_2017,df_2018,df_2019]:\n",
    "for index, row in df.iloc[78100:,:].iterrows():\n",
    "    id = row[\"ID\"]\n",
    "    init = row[\"start\"]\n",
    "    end = row[\"end\"]\n",
    "    location = row[\"location\"]\n",
    "    list_dayofcount = [value.strftime(\"%Y-%m-%d\") for value in pd.date_range(start=init,end=end).to_list()]\n",
    "    for day in list_dayofcount:\n",
    "        try:\n",
    "            df_day = pd.read_csv(os.path.join(\"../Join/telemetriedaten/position_per_wild_fiwi/\" + id + \"/\",day + \"_fiwi.csv\"))\n",
    "            df_day[\"Date\"] = pd.to_datetime(df_day[\"Date\"], infer_datetime_format=True)   \n",
    "            df_to_save = df_day[(df_day[\"Date\"] > init) & (df_day[\"Date\"] < end)][[\"ID\",\"Date\",\"x-value\", \"y-value\"]]\n",
    "            df_to_save['location'] = location\n",
    "            df_final = pd.concat([df_final,df_to_save.iloc[:,:]])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "df_final.to_csv(f\"../Join/telemetriedaten/areas.csv\")\n",
    "    # df_final.to_csv(f\"../Join/telemetriedaten/areas_{index_data+16}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"../Join/telemetriedaten/areas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fechas juntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wild board DE.2011.14\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir(\"../Join/telemetriedaten/position_per_wild_fiwi/DE.2011.14/\"):\n",
    "    print(file)\n",
    "    df_day = pd.read_csv(\"../Join/telemetriedaten/position_per_wild_fiwi/DE.2011.14/\" + file)\n",
    "    df_day = df_day[['Date', 'tagid', 'MACadresse', 'x-value', 'y-value', 'ID', 'TempMain', \n",
    "                      'HumMain', 'Wind','WindDir', 'Rain', 'Solar', 'TempBB', 'TempForest', 'HumForest',\n",
    "                      'BaroPressure', '°C_FG', 'hum_FG', 'dew_p_FG', '°C_VG', 'hum_VG', 'dew_p_VG', 'SuhlenVG', 'SuhlenFG']]\n",
    "    df = pd.concat([df,df_day.iloc[:,:]])\n",
    "df.to_csv(\"../Join/telemetriedaten/position_per_wild_fiwi/DE.2011.14/DE.2011.14.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_467/1405016587.py:3: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path_file)\n"
     ]
    }
   ],
   "source": [
    "# Wild board DE.2011.14\n",
    "path_file = \"../Join/telemetriedaten/position_per_wild_fiwi/DE.2011.14/DE.2011.14.csv\"\n",
    "df = pd.read_csv(path_file)\n",
    "df = df.set_index('Date')\n",
    "df = df.drop(columns=[\"Unnamed: 0\",\"tagid\",\"MACadresse\",\"ID\"])\n",
    "ohe = pd.get_dummies(df.WindDir)\n",
    "df = df.drop('WindDir',axis = 1)\n",
    "# df = df.join(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df[['TempMain', 'HumMain', 'Wind', 'Rain', 'Solar',\n",
    "       'TempBB', 'TempForest', 'HumForest', 'BaroPressure', '°C_FG', 'hum_FG',\n",
    "       'dew_p_FG', '°C_VG', 'hum_VG', 'dew_p_VG', 'SuhlenVG', 'SuhlenFG']] = scaler.fit_transform(df[['TempMain', 'HumMain', 'Wind', 'Rain', 'Solar',\n",
    "       'TempBB', 'TempForest', 'HumForest', 'BaroPressure', '°C_FG', 'hum_FG',\n",
    "       'dew_p_FG', '°C_VG', 'hum_VG', 'dew_p_VG', 'SuhlenVG', 'SuhlenFG']])\n",
    "values = df.values\n",
    "values = values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_467/1790526574.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temporal[[\"x-value\",\"y-value\"]] = df_temporal[[\"x-value\",\"y-value\"]].shift(1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x-value</th>\n",
       "      <th>y-value</th>\n",
       "      <th>TempMain</th>\n",
       "      <th>HumMain</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Solar</th>\n",
       "      <th>TempBB</th>\n",
       "      <th>TempForest</th>\n",
       "      <th>HumForest</th>\n",
       "      <th>BaroPressure</th>\n",
       "      <th>°C_FG</th>\n",
       "      <th>hum_FG</th>\n",
       "      <th>dew_p_FG</th>\n",
       "      <th>°C_VG</th>\n",
       "      <th>hum_VG</th>\n",
       "      <th>dew_p_VG</th>\n",
       "      <th>SuhlenVG</th>\n",
       "      <th>SuhlenFG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-02-01 17:34:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576842</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.186147</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 17:34:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576842</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.186147</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 17:36:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576842</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.186147</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01 17:37:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576842</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482201</td>\n",
       "      <td>0.186147</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     x-value  y-value  TempMain   HumMain  Wind  Rain  Solar  \\\n",
       "Date                                                                           \n",
       "2017-02-01 17:34:00      0.0      0.0  0.576842  0.253333   0.0   0.0    0.0   \n",
       "2017-02-01 17:34:00      0.0      0.0  0.576842  0.253333   0.0   0.0    0.0   \n",
       "2017-02-01 17:36:00      0.0      0.0  0.576842  0.253333   0.0   0.0    0.0   \n",
       "2017-02-01 17:37:00      0.0      0.0  0.576842  0.253333   0.0   0.0    0.0   \n",
       "\n",
       "                       TempBB  TempForest  HumForest  BaroPressure  °C_FG  \\\n",
       "Date                                                                        \n",
       "2017-02-01 17:34:00  0.482201    0.186147   0.931507      0.574899    NaN   \n",
       "2017-02-01 17:34:00  0.482201    0.186147   0.931507      0.574899    NaN   \n",
       "2017-02-01 17:36:00  0.482201    0.186147   0.931507      0.574899    NaN   \n",
       "2017-02-01 17:37:00  0.482201    0.186147   0.931507      0.574899    NaN   \n",
       "\n",
       "                     hum_FG  dew_p_FG  °C_VG  hum_VG  dew_p_VG  SuhlenVG  \\\n",
       "Date                                                                       \n",
       "2017-02-01 17:34:00     NaN       NaN    NaN     NaN       NaN       NaN   \n",
       "2017-02-01 17:34:00     NaN       NaN    NaN     NaN       NaN       NaN   \n",
       "2017-02-01 17:36:00     NaN       NaN    NaN     NaN       NaN       NaN   \n",
       "2017-02-01 17:37:00     NaN       NaN    NaN     NaN       NaN       NaN   \n",
       "\n",
       "                     SuhlenFG  \n",
       "Date                           \n",
       "2017-02-01 17:34:00       NaN  \n",
       "2017-02-01 17:34:00       NaN  \n",
       "2017-02-01 17:36:00       NaN  \n",
       "2017-02-01 17:37:00       NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temporal = df.iloc[0:5,:]\n",
    "df_temporal[[\"x-value\",\"y-value\"]] = df_temporal[[\"x-value\",\"y-value\"]].shift(1)\n",
    "df_temporal = df_temporal.dropna(subset=[\"x-value\",\"y-value\"])\n",
    "df_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMs are sensitive to the scale of the input data\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "train = None\n",
    "validation = None\n",
    "test = None\n",
    "\n",
    "X_train, y_train = None,None\n",
    "X_validation, y_validation = None,None\n",
    "X_test, y_test = None,None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and miscelaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=100,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=5),\n",
    "                ModelCheckpoint('../models/model.h5', save_best_only=True, save_weights_only=False)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,  y_train, \n",
    "                    batch_size=2048, epochs=150,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "002eb5a802fb8e476512fc9f67734d3aea06c712464ddb2bc5f18fbbbc19b494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
